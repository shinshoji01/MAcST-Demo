{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./audio/\"\n",
    "transcription_dir = \"../seq2seq-vc/datasetgeneration/LLM_responses/08-A_multi-lingual_text/\"\n",
    "other_dir = \"../seq2seq-vc/datasetgeneration/LLM_responses/08-Others_multi-lingual_text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            <h5 class=\"mb-4\">Sho Inoue<sup>1,2,3</sup>, Shuai Wang<sup>2†</sup>, Wanxing Wang<sup>3</sup>, Pengcheng Zhu<sup>3</sup>, Mengxiao Bi<sup>3</sup>, Haizhou Li<sup>1,2</sup></h5>\n",
    "              <p>\n",
    "                <sup>1</sup>School of Data Science, Shenzhen Research Institute of Big Data<br>\n",
    "                <sup>2</sup>The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China<br>\n",
    "                <sup>3</sup>Fuxi AI Lab, NetEase Inc., Hangzhou, China\n",
    "              </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "          <figure class=\"text-center\">\n",
    "            <img src=\"{fig_path}\" alt=\"overall diagram of the pipeline\" class=\"img-fluid\" style=\"width: 900px; height: auto;\">\n",
    "          </figure>\n",
    "          <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "webtitle = \"MacST Demo Page\"\n",
    "title = \"MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent Conversion\"\n",
    "abstract = \"In accented voice conversion or accent conversion, we seek to convert the accent in speech from one another while preserving speaker identity and semantic content. In this study, we formulate a novel method for creating multi-accented speech samples, thus pairs of accented speech samples by the same speaker, through text transliteration for training accent conversion systems. We begin by generating transliterated text with a Large Language Model (LLM), which are then fed into multilingual TTS models to synthesize accented English speech. As a reference system, we built a sequence-to-sequence model on the synthetic parallel corpus for accent conversion. We validated the proposed method for both native and non-native English speakers. Subjective and objective evaluations further validate our dataset's effectiveness in accent conversion studies. \"\n",
    "github_url = \"https://github.com/shinshoji01/MacST-project-page\"\n",
    "# base_repo_dir = \"/\"\n",
    "base_repo_dir = \"/MacST-Demo/\"\n",
    "style_dir = base_repo_dir + \"statics/\"\n",
    "fig_path = base_repo_dir + \"images/diagram.svg\"\n",
    "\n",
    "initial = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\">\n",
    "    <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n",
    "    <title>{webtitle}</title>\n",
    "    <link href=\"{style_dir}bootstrap-5.2.3-dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "    <link href=\"{style_dir}my.css\" rel=\"stylesheet\">\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"container\">\n",
    "      <div class=\"row\">\n",
    "        <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "          <div class=\"text-center\">\n",
    "            <h2>{title}</h2>\n",
    "            <br>\n",
    "            <h5 class=\"mb-4\">Sho Inoue<sup>1,2,3</sup>, Shuai Wang<sup>2†</sup>, Wanxing Wang<sup>3</sup>, Pengcheng Zhu<sup>3</sup>, Mengxiao Bi<sup>3</sup>, Haizhou Li<sup>1,2</sup></h5>\n",
    "              <p>\n",
    "                <sup>1</sup>School of Data Science, Shenzhen Research Institute of Big Data<br>\n",
    "                <sup>2</sup>The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China<br>\n",
    "                <sup>3</sup>Fuxi AI Lab, NetEase Inc., Hangzhou, China\n",
    "              </p>\n",
    "          </div>\n",
    "          <br>\n",
    "          <figure class=\"text-center\">\n",
    "            <img src=\"{fig_path}\" alt=\"overall diagram of the pipeline\" class=\"img-fluid\" style=\"width: 900px; height: auto;\">\n",
    "          </figure>\n",
    "          <br>\n",
    "          <h3>Abstract</h3>\n",
    "          <p class=\"lead\">\n",
    "          {abstract}\n",
    "          </p>\n",
    "          <p class=\"lead\">You can visit the project page of this paper: <a href=\"{github_url}\">Github Repository</a>.\n",
    "          </p>\n",
    "        </div>\n",
    "\"\"\"[1:]\n",
    "\n",
    "closure = f\"\"\"\n",
    "      </div>\n",
    "    </div>\n",
    "    <script src=\"{style_dir}jquery/jquery-3.7.1.slim.min.js\"></script>\n",
    "    <script src=\"{style_dir}bootstrap-5.2.3-dist/bootstrap.min.js\"></script>\n",
    "\"\"\"[1:]\n",
    "closure += \"\"\"\n",
    "  </body>\n",
    "  <script>\n",
    "    $(function(){\n",
    "        $(\"audio\").on(\"play\", function() {\n",
    "            $(\"audio\").not(this).each(function(index, audio) {\n",
    "                audio.pause();\n",
    "                audio.currentTime = 0;\n",
    "            });\n",
    "        });\n",
    "    });\n",
    "    </script>\n",
    "</html>\n",
    "\"\"\"[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"hindi\": {\n",
    "        \"CMU-ARCTIC/SLT\": \"Ground Truth (SLT/American)\",\n",
    "        \"PD-AST/SLT/English\": \"MacST (SLT/American)\",\n",
    "        \"PD-AST/SLT/Hindi\": \"MacST (SLT/Hindi)\",\n",
    "        \"L2-ARCTIC/ASI\": \"Ground Truth (ASI/Hindi)\",\n",
    "        \"PD-AST/ASI/Hindi\": \"MacST (ASI/Hindi)\",\n",
    "        \"L2-ARCTIC/TNI\": \"Ground Truth (TNI/Hindi)\",\n",
    "        \"PD-AST/TNI/Hindi\": \"MacST (TNI/Hindi)\",\n",
    "    },\n",
    "    \"korean\": {\n",
    "        \"CMU-ARCTIC/SLT\": \"Ground Truth (SLT/American)\",\n",
    "        \"PD-AST/SLT/English\": \"MacST (SLT/American)\",\n",
    "        \"PD-AST/SLT/Korean\": \"MacST (SLT/Korean)\",\n",
    "        \"L2-ARCTIC/HKK\": \"Ground Truth (HKK/Korean)\",\n",
    "        \"PD-AST/HKK/Korean\": \"MacST (HKK/Korean)\",\n",
    "        \"L2-ARCTIC/YDCK\": \"Ground Truth (YDCK/Korean)\",\n",
    "        \"PD-AST/YDCK/Korean\": \"MacST (YDCK/Korean)\",\n",
    "    },\n",
    "    \"ac\": {\n",
    "        \"CMU-ARCTIC/SLT\": \"Ground Truth (SLT/American/source)\",\n",
    "        \"PD-AST/SLT/English\": \"MacST (SLT/American)\",\n",
    "        \"PD-AST/SLT/Hindi\": \"MacST (SLT/Hindi/target)\",\n",
    "        \"VTN_fine-tuning_nocondition_gt2syn_hifiganmelhifiganmel_hubert_norepeating/100000\": \"AC w/o Data Augmentation\",\n",
    "        \"VTN_fine-tuning_nocondition_mix2synVCTK3hr_hifiganmelhifiganmel_hubert_norepeating/100000\": \"AC w/ Data Augmentation (ours)\",\n",
    "    },\n",
    "    \"others\": {\n",
    "        \"Others/English\": \"American\",\n",
    "        \"Others/Hindi\": \"Hindi\",\n",
    "        \"Others/Korean\": \"Korean\",\n",
    "        \"Others/Japanese\": \"Japanese\",\n",
    "        \"Others/Russian\": \"Russian\",\n",
    "        \"Others/Arabic\": \"Arabic\",\n",
    "        # \"Others/French\": \"French\",\n",
    "        # \"Others/Mandarin\": \"Mandarin\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = [os.path.basename(a)[:-4] for a in glob.glob(base_dir + \"CMU-ARCTIC___SLT/*\")]\n",
    "filenames = ['arctic_a0024', 'arctic_a0029', 'arctic_a0058', 'arctic_a0073', 'arctic_a0085', 'arctic_a0092', 'arctic_a0099', 'arctic_a0131', 'arctic_a0152', 'arctic_a0170', 'arctic_a0202', 'arctic_a0210', 'arctic_a0245', 'arctic_a0258', 'arctic_a0274', 'arctic_a0315', 'arctic_a0369', 'arctic_a0374', 'arctic_a0378', 'arctic_a0384', 'arctic_a0389', 'arctic_a0449', 'arctic_a0544', 'arctic_a0545', 'arctic_a0552', 'arctic_a0556', 'arctic_a0561', 'arctic_b0008', 'arctic_b0019', 'arctic_b0047', 'arctic_b0067', 'arctic_b0083', 'arctic_b0113', 'arctic_b0142', 'arctic_b0147', 'arctic_b0163', 'arctic_b0190', 'arctic_b0210', 'arctic_b0224', 'arctic_b0253', 'arctic_b0257', 'arctic_b0296', 'arctic_b0317', 'arctic_b0318', 'arctic_b0339', 'arctic_b0372', 'arctic_b0380', 'arctic_b0381', 'arctic_b0469', 'arctic_b0474', 'arctic_b0497', 'arctic_b0508']\n",
    "np.random.seed(0)\n",
    "filenames = list(np.random.choice(filenames, 20, replace=False))\n",
    "filenames.sort()\n",
    "otherfns = ['arctic_a0058', 'arctic_a0085', 'arctic_a0210', 'arctic_a0561', 'arctic_b0019']\n",
    "otherfns.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\n",
    "    \"hindi\": \"Speech Samples of MacST (Hindi Accent)\",\n",
    "    \"korean\": \"Speech Samples of MacST (Korean Accent)\",\n",
    "    \"ac\": \"Speech Samples of Accent Conversion (Hindi Accent)\",\n",
    "    \"others\": \"Other Accents from MacST\",\n",
    "}\n",
    "explanations = {\n",
    "    \"hindi\": \"\\\n",
    "This section includes audio samples generated by our proposed generation method, MacST. \\\n",
    "There are three speakers involved in this section.<br> \\\n",
    "<ul> \\\n",
    "  <li><b>SLT</b>: American speaker from CMU-ARCTIC <br></li> \\\n",
    "  <li><b>ASI</b>: Hindi speaker from L2-ARCTIC <br></li> \\\n",
    "  <li><b>TNI</b>: Hindi speaker from L2-ARCTIC <br></li> \\\n",
    "</ul> \\\n",
    "In MacST, the languages in curly brackets indicate the transliteration languages. Each column contains the original transcription and its Hindi-transliterated text.\\\n",
    "\",\n",
    "    \"korean\": \"\\\n",
    "This section includes audio samples generated by our proposed generation method, MacST. \\\n",
    "There are three speakers involved in this section.<br> \\\n",
    "<ul> \\\n",
    "  <li><b>SLT</b>: American speaker from CMU-ARCTIC <br></li> \\\n",
    "  <li><b>HKK</b>: Korean speaker from L2-ARCTIC <br></li> \\\n",
    "  <li><b>YDCK</b>: Korean speaker from L2-ARCTIC <br></li> \\\n",
    "</ul> \\\n",
    "In MacST, the languages in curly brackets indicate the transliteration languages. Each column contains the original transcription and its Korean-transliterated text.\\\n",
    "\",\n",
    "    \"ac\": \"\\\n",
    "This section includes audio samples generated by Accent Conversion (AC) Models in Hindi Accent. \\\n",
    "The input is a native accent while the output is a Hindi accent. \\\n",
    "We used SLT for the whole experiment. \\\n",
    "We compared two models: \\\n",
    "the first (<b>AC w/o Data Augmentation</b>) was trained on paired data with ground-truth input from CMU-ARCTIC and synthetic target output from MacST; \\\n",
    "the second model (<b>AC w/ Data Augmentation (ours)</b>) incorporated additional synthetic speech pairs from MacST. We augmented our dataset with 1 hour from ARCTIC transcriptions and an extra 3 hours from VCTK to generate American and Hindi-accented speech pairs. \\\n",
    "\",\n",
    "    \"others\": \"\\\n",
    "This section includes audio samples of additional accents, generated by MacST including Japanese, Russian, and Arabic. \\\n",
    "We used a speaker provided by <a href='https://elevenlabs.io/'>11ElevenLabs</a>. \\\n",
    "Each audio sample is accompanied by the transliterated text corresponding to the accent. \\\n",
    "\",\n",
    "}\n",
    "accents = {\n",
    "    \"hindi\": \"Hindi\",\n",
    "    \"korean\": \"Korean\",\n",
    "    \"ac\": \"Hindi\",\n",
    "    \"others\": False,\n",
    "}\n",
    "textincell = {\n",
    "    \"hindi\": False,\n",
    "    \"korean\": False,\n",
    "    \"ac\": False,\n",
    "    \"others\": True,\n",
    "}\n",
    "filenames_dir = {\n",
    "    \"hindi\": filenames,\n",
    "    \"korean\": filenames,\n",
    "    \"ac\": filenames,\n",
    "    \"others\": filenames,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extexts = {}\n",
    "for exid in experiments:\n",
    "    text = f\"\"\"\n",
    "            <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "              <h3>{titles[exid]}</h3>\n",
    "              <p class=\"lead\">\n",
    "                    {explanations[exid]}\n",
    "                    </p>\n",
    "              <div class=\"table-responsive\" style=\"overflow-x: scroll\">\n",
    "                <table class=\"table table-sm\">\n",
    "    \"\"\"[1:]\n",
    "    text += f\"\"\"\n",
    "                  <thead>\n",
    "                    <tr>\n",
    "                      <th scope=\"col\">ID</th>\n",
    "    \"\"\"[1:]\n",
    "    for fn in filenames_dir[exid]:\n",
    "        text += f\"\"\"\n",
    "                      <th scope=\"col\">{fn}</th>\n",
    "    \"\"\"[1:]\n",
    "    text += \"\"\"\n",
    "                    </tr>\n",
    "                  </thead>\n",
    "    \"\"\"[1:]\n",
    "    headerfn = text\n",
    "\n",
    "    text = \"\"\"\n",
    "                  <tbody>\n",
    "                    <tr>\n",
    "                      <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">Text</th>\n",
    "    \"\"\"[1:]\n",
    "    for fn in filenames_dir[exid]:\n",
    "        data = np.load(transcription_dir + f\"{fn}.npy\", allow_pickle=True).item()\n",
    "        if accents[exid]:\n",
    "            text += f\"\"\"\n",
    "                      <td>\n",
    "                        <p><b>Original</b>: {data['Original English']}<br><br><b>Transliterated</b>: {data[accents[exid]]}</p>\n",
    "                      </td>\n",
    "    \"\"\"[1:]\n",
    "        else:\n",
    "            text += f\"\"\"\n",
    "                      <td>\n",
    "                        <p>{data['Original English']}</p>\n",
    "                      </td>\n",
    "    \"\"\"[1:]\n",
    "    text += \"\"\"\n",
    "                    </tr>\n",
    "    \"\"\"[1:]\n",
    "    headertext = text\n",
    "\n",
    "    text = \"\"\n",
    "    for key in experiments[exid]:\n",
    "        text += f\"\"\"\n",
    "                    <tr>\n",
    "                      <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">{experiments[exid][key]}</th>\n",
    "    \"\"\"[1:]\n",
    "        for fn in filenames_dir[exid]:\n",
    "            wavfile = base_dir + \"___\".join(key.split(\"/\")) + f\"/{fn}.wav\"\n",
    "            text += f\"\"\"\n",
    "                      <td>\n",
    "    \"\"\"[1:]\n",
    "            text += f\"\"\"\n",
    "                        <audio controls=\"\" preload=\"none\" style=\"width: 240px\">\n",
    "                          <source src=\"{wavfile}\" type=\"audio/wav\">\n",
    "                        </audio>\n",
    "    \"\"\"[1:]\n",
    "            if textincell[exid]:\n",
    "                data = np.load(other_dir + f\"{fn}.npy\", allow_pickle=True).item()\n",
    "                integrated_text = data[key.split(\"/\")[-1]]\n",
    "                text += f\"\"\"\n",
    "                        <p>{integrated_text}</p>\n",
    "    \"\"\"[1:]\n",
    "            text += f\"\"\"\n",
    "                      </td>\n",
    "    \"\"\"[1:]\n",
    "        text += \"\"\"\n",
    "                    </tr>\n",
    "    \"\"\"[1:]\n",
    "    body = text\n",
    "\n",
    "    text = \"\"\"\n",
    "                  </tbody>\n",
    "                </table>\n",
    "              </div>\n",
    "              <p class=\"lead\">* please scroll horizontally to explore additional columns in the table.</p>\n",
    "            </div>\n",
    "    \"\"\"[1:]\n",
    "    tableclosure = text\n",
    "    extexts[exid] = headerfn + headertext + body + tableclosure\n",
    "    extexts[exid] = \"\\n\".join([a[4:] for a in extexts[exid].split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholetext = \"\"\n",
    "wholetext += initial\n",
    "for exid in experiments:\n",
    "    wholetext += extexts[exid]\n",
    "wholetext += closure\n",
    "f = open(\"index.html\", \"w\")\n",
    "f.write(wholetext)\n",
    "f.close()\n",
    "print(wholetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
